{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class BagOfWords:\n",
    "    def __init__(self, stopwords=None):\n",
    "        \"\"\"\n",
    "        Initializes the Bag of Words model.\n",
    "\n",
    "        Parameters:\n",
    "        - stopwords (set): A set of words to ignore. Default is None.\n",
    "        \"\"\"\n",
    "        self.stopwords = stopwords if stopwords else set()\n",
    "        self.vocab = set()\n",
    "        self.word_to_index = {}\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses the input text (tokenization, lowercasing, removing\n",
    "        stopwords).\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The raw text string.\n",
    "\n",
    "        Returns:\n",
    "        - tokens (list): A list of preprocessed tokens.\n",
    "        \"\"\"\n",
    "        # Lowercase the text and remove special characters\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\W+\", \" \", text)\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in self.stopwords]\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary from a corpus of documents.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents (each document is a\n",
    "        string).\n",
    "        \"\"\"\n",
    "        for document in corpus:\n",
    "            tokens = self.preprocess(document)\n",
    "            self.vocab.update(tokens)\n",
    "\n",
    "        # Assign an index to each unique word\n",
    "        self.word_to_index = {w: i for i, w in enumerate(sorted(self.vocab))}\n",
    "\n",
    "    def vectorize(self, document):\n",
    "        \"\"\"\n",
    "        Converts a document into its Bag of Words vector representation.\n",
    "\n",
    "        Parameters:\n",
    "        - document (str): A single document (string).\n",
    "\n",
    "        Returns:\n",
    "        - vector (list): A list representing the BoW vector for the document.\n",
    "        \"\"\"\n",
    "        tokens = self.preprocess(document)\n",
    "        vector = [0] * len(self.vocab)\n",
    "\n",
    "        token_counts = Counter(tokens)\n",
    "        for token, count in token_counts.items():\n",
    "            if token in self.word_to_index:\n",
    "                index = self.word_to_index[token]\n",
    "                vector[index] = count\n",
    "\n",
    "        return vector\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        \"\"\"\n",
    "        Transforms a corpus into BoW vectors.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents.\n",
    "\n",
    "        Returns:\n",
    "        - vectors (list of list): A list of BoW vectors.\n",
    "        \"\"\"\n",
    "        return [self.vectorize(doc) for doc in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'brown': 0, 'dog': 1, 'enemy': 2, 'fox': 3, 'gunboats': 4, 'jeopardize': 5, 'jump': 6, 'jumps': 7, 'lazy': 8, 'movement': 9, 'never': 10, 'of': 11, 'quick': 12, 'quickly': 13, 'six': 14}\n",
      "\n",
      "Vectors:\n",
      "Document 1: [1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]\n",
      "Document 2: [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0]\n",
      "Document 3: [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Never jump over the lazy dog quickly\",\n",
    "    \"A quick movement of the enemy will jeopardize six gunboats\"\n",
    "]\n",
    "\n",
    "# Initialize Bag of Words model with some stopwords\n",
    "stopwords = {\"the\", \"over\", \"a\", \"will\"}\n",
    "bow_model = BagOfWords(stopwords=stopwords)\n",
    "\n",
    "# Build vocabulary and vectorize the corpus\n",
    "bow_model.build_vocab(corpus)\n",
    "vectors = bow_model.transform(corpus)\n",
    "\n",
    "# Print vocabulary and vectors\n",
    "print(\"Vocabulary:\", bow_model.word_to_index)\n",
    "print(\"\\nVectors:\")\n",
    "for i, vector in enumerate(vectors):\n",
    "    print(f\"Document {i+1}: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "why does bag of word return the frequency of the word in the final vector?\n",
    "\n",
    "The Bag of Words (BoW) model represents text data in a way that's easy for machine learning algorithms to work with. Here's why it specifically uses word frequency in the final vector:\n",
    "\n",
    "### 1. Capturing the Importance of Words\n",
    "- Frequency reflects how important a word is in a given text. Words that appear more frequently in a document are likely more relevant to the content of that document.\n",
    "- For example, in a document about \"sports,\" words like \"game,\" \"team,\" and \"player\" may appear more often. By counting their frequency, the model captures these as important features.\n",
    "### 2. Simplicity and Interpretability\n",
    "BoW is a simple and interpretable approach:\n",
    "- Each dimension in the vector corresponds to a specific word from the vocabulary.\n",
    "- The value in each dimension is the number of times that word appears in the document.\n",
    "Example: For a vocabulary of ['apple', 'banana', 'orange']:\n",
    "Document: \"apple banana apple\"\n",
    "Vector: [2, 1, 0] (2 occurrences of \"apple\", 1 of \"banana\", and 0 of \"orange\").\n",
    "- This representation makes it straightforward to use as input for algorithms.\n",
    "### 3. Preserving Contextual Weight in the Document\n",
    "- High frequency words have a stronger impact.\n",
    "- Algorithms (like Naive Bayes or logistic regression) can leverage word counts to predict relationships between certain words and specific outcomes, such as classifying documents by topic or sentiment.\n",
    "### 4. Compatibility with Machine Learning\n",
    "- Machine learning models often work with numerical data. By converting text into vectors of word frequencies:\n",
    "- The algorithms can treat text data just like any other numeric feature.\n",
    "- Word frequency provides an initial way to quantify text in a meaningful manner.\n",
    "### 5. Why Frequency and Not Just Presence?\n",
    "Using frequency (instead of just 1 or 0 to indicate presence/absence) allows the model to:\n",
    "- Weigh more frequent words higher.\n",
    "- Differentiate between documents with similar words but varying amounts of repetition.\n",
    "- Presence-only (binary) vectors lose information about the importance conveyed by repetition:\n",
    "E.g., \"I love cats\" vs. \"I love cats cats cats\" would both result in [1, 1] using binary, but [1, 3] in frequency-based BoW.\n",
    "\n",
    "### Potential Extensions of BoW:\n",
    "TF-IDF: Extends BoW by normalizing word frequency based on how commonly the word appears across all documents. This reduces the importance of very common words (like \"the,\" \"and\").\n",
    "In summary, BoW uses word frequency because it captures the importance of words in a document and provides a simple numeric representation of text that is effective for many basic machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
