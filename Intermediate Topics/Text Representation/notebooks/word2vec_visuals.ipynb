{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Overview\n",
    "Word2Vec is a model introduced by Mikolov et al. (2013) for learning dense, low-dimensional vector representations (embeddings) of words. The model captures semantic and syntactic relationships between words based on their co-occurrence in a corpus.\n",
    "\n",
    "Word2Vec offers two main architectures for training:\n",
    "\n",
    "### 1. Skip-gram Model\n",
    "Objective: Given a target word, predict the surrounding context words.\n",
    "\n",
    "Training Data: (Target, Context) pairs.\n",
    "\n",
    "Mathematical Formulation:\n",
    "\n",
    "Maximize the probability of context words given a target word:\n",
    "```\n",
    "𝑃(𝑐𝑜𝑛𝑡𝑒𝑥𝑡∣𝑡𝑎𝑟𝑔𝑒𝑡)=∏𝑤𝑐∈context𝑃(𝑤𝑐∣𝑤𝑡)\n",
    "```\n",
    "\n",
    "The model optimizes the likelihood that a word 𝑤_𝑐 appears in the context window of the target word 𝑤_𝑡.\n",
    "\n",
    "Characteristics:\n",
    "- Generates more training samples per word, as it creates a pair for each target-context combination.\n",
    "- Works well for rare words, as it trains on every occurrence of the target word independently.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "- Tasks where capturing detailed, fine-grained relationships between words is important.\n",
    "- Smaller datasets, where learning high-quality embeddings for rare words is crucial.\n",
    "\n",
    "### 2. CBOW (Continuous Bag of Words) Model\n",
    "Objective: Given a set of context words, predict the target word.\n",
    "\n",
    "Training Data: (Context, Target) pairs.\n",
    "\n",
    "Mathematical Formulation:\n",
    "\n",
    "Maximize the probability of the target word given its context words:\n",
    "```\n",
    "𝑃(𝑡𝑎𝑟𝑔𝑒𝑡∣𝑐𝑜𝑛𝑡𝑒𝑥𝑡)=𝑃(𝑤𝑡∣𝑤𝑐1,𝑤𝑐2,…,𝑤𝑐𝑛)\n",
    "```\n",
    "\n",
    "The model optimizes the likelihood that a word 𝑤𝑡 is the center of a given set of context words.\n",
    "\n",
    "Characteristics:\n",
    "- Aggregates multiple context words to predict the target, which results in fewer training samples compared to Skip-gram.\n",
    "- Trains faster and is more efficient for frequent words.\n",
    "\n",
    "Use Cases:\n",
    "- Tasks where speed and computational efficiency are important.\n",
    "- Large datasets, where learning embeddings quickly for common words suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Two Architectures?\n",
    "The choice of Skip-gram or CBOW depends on:\n",
    "\n",
    "#### Dataset Size:\n",
    "- Skip-gram works better with small datasets and rare words.\n",
    "- CBOW is faster and works well with large datasets.\n",
    "\n",
    "#### Task Requirements:\n",
    "- Use Skip-gram if capturing fine-grained semantic relationships is crucial.\n",
    "- Use CBOW for faster, general-purpose embeddings.\n",
    "\n",
    "#### Computational Resources:\n",
    "- CBOW requires less computational power and trains faster.\n",
    "- Skip-gram can be more computationally intensive due to the larger number of training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_pairs(corpus, word_to_index, window_size=2, model=\"skipgram\"):\n",
    "    \"\"\"\n",
    "    Generates and displays training pairs for Skip-gram and CBOW models.\n",
    "    \n",
    "    Parameters:\n",
    "    - corpus (list of str): A list of documents (each document is a string).\n",
    "    - word_to_index (dict): A dictionary mapping words to their indices.\n",
    "    - window_size (int): The size of the context window.\n",
    "    - model (str): The model type ('skipgram' or 'cbow').\n",
    "    \n",
    "    Returns:\n",
    "    - pairs (list of tuples): A list of training pairs for the specified model.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    # Tokenize the corpus and generate pairs\n",
    "    for doc in corpus:\n",
    "        tokens = [word for word in re.findall(r'\\w+', doc.lower()) if word in word_to_index]\n",
    "        for idx, word in enumerate(tokens):\n",
    "            start = max(0, idx - window_size)\n",
    "            end = min(len(tokens), idx + window_size + 1)\n",
    "            context = tokens[start:idx] + tokens[idx + 1:end]\n",
    "            \n",
    "            if model == \"skipgram\":\n",
    "                # Skip-gram: (target, context)\n",
    "                pairs.extend([(word, context_word) for context_word in context])\n",
    "            elif model == \"cbow\":\n",
    "                # CBOW: (context, target)\n",
    "                pairs.append((context, word))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model type. Choose 'skipgram' or 'cbow'.\")\n",
    "    \n",
    "    # Display pairs\n",
    "    for pair in pairs[:10]:  # Display only the first 10 pairs for brevity\n",
    "        print(pair)\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Skip-gram Training Pairs ---\n",
      "('the', 'quick')\n",
      "('the', 'brown')\n",
      "('the', 'fox')\n",
      "('quick', 'the')\n",
      "('quick', 'brown')\n",
      "('quick', 'fox')\n",
      "('quick', 'jumps')\n",
      "('brown', 'the')\n",
      "('brown', 'quick')\n",
      "('brown', 'fox')\n",
      "\n",
      "--- CBOW Training Pairs ---\n",
      "(['quick', 'brown', 'fox'], 'the')\n",
      "(['the', 'brown', 'fox', 'jumps'], 'quick')\n",
      "(['the', 'quick', 'fox', 'jumps', 'over'], 'brown')\n",
      "(['the', 'quick', 'brown', 'jumps', 'over', 'the'], 'fox')\n",
      "(['quick', 'brown', 'fox', 'over', 'the', 'lazy'], 'jumps')\n",
      "(['brown', 'fox', 'jumps', 'the', 'lazy', 'dog'], 'over')\n",
      "(['fox', 'jumps', 'over', 'lazy', 'dog'], 'the')\n",
      "(['jumps', 'over', 'the', 'dog'], 'lazy')\n",
      "(['over', 'the', 'lazy'], 'dog')\n",
      "(['jump', 'over', 'the'], 'never')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['quick', 'brown', 'fox'], 'the'),\n",
       " (['the', 'brown', 'fox', 'jumps'], 'quick'),\n",
       " (['the', 'quick', 'fox', 'jumps', 'over'], 'brown'),\n",
       " (['the', 'quick', 'brown', 'jumps', 'over', 'the'], 'fox'),\n",
       " (['quick', 'brown', 'fox', 'over', 'the', 'lazy'], 'jumps'),\n",
       " (['brown', 'fox', 'jumps', 'the', 'lazy', 'dog'], 'over'),\n",
       " (['fox', 'jumps', 'over', 'lazy', 'dog'], 'the'),\n",
       " (['jumps', 'over', 'the', 'dog'], 'lazy'),\n",
       " (['over', 'the', 'lazy'], 'dog'),\n",
       " (['jump', 'over', 'the'], 'never'),\n",
       " (['never', 'over', 'the', 'lazy'], 'jump'),\n",
       " (['never', 'jump', 'the', 'lazy', 'dog'], 'over'),\n",
       " (['never', 'jump', 'over', 'lazy', 'dog', 'quickly'], 'the'),\n",
       " (['jump', 'over', 'the', 'dog', 'quickly'], 'lazy'),\n",
       " (['over', 'the', 'lazy', 'quickly'], 'dog'),\n",
       " (['the', 'lazy', 'dog'], 'quickly'),\n",
       " (['movement', 'the', 'enemy'], 'quick'),\n",
       " (['quick', 'the', 'enemy', 'jeopardize'], 'movement'),\n",
       " (['quick', 'movement', 'enemy', 'jeopardize', 'six'], 'the'),\n",
       " (['quick', 'movement', 'the', 'jeopardize', 'six', 'gunboats'], 'enemy'),\n",
       " (['movement', 'the', 'enemy', 'six', 'gunboats'], 'jeopardize'),\n",
       " (['the', 'enemy', 'jeopardize', 'gunboats'], 'six'),\n",
       " (['enemy', 'jeopardize', 'six'], 'gunboats')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Never jump over the lazy dog quickly\",\n",
    "    \"A quick movement of the enemy will jeopardize six gunboats\"\n",
    "]\n",
    "\n",
    "# Example vocabulary and word-to-index mapping\n",
    "word_to_index = {\n",
    "    \"the\": 0, \"quick\": 1, \"brown\": 2, \"fox\": 3, \"jumps\": 4, \n",
    "    \"over\": 5, \"lazy\": 6, \"dog\": 7, \"never\": 8, \"jump\": 9,\n",
    "    \"quickly\": 10, \"movement\": 11, \"enemy\": 12, \"jeopardize\": 13,\n",
    "    \"six\": 14, \"gunboats\": 15\n",
    "}\n",
    "\n",
    "print(\"\\n--- Skip-gram Training Pairs ---\")\n",
    "generate_training_pairs(corpus, word_to_index, window_size=3, model=\"skipgram\")\n",
    "\n",
    "print(\"\\n--- CBOW Training Pairs ---\")\n",
    "generate_training_pairs(corpus, word_to_index, window_size=3, model=\"cbow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, vector_size=50, window_size=2, learning_rate=0.01, epochs=10, negative_samples=5):\n",
    "        \"\"\"\n",
    "        Initializes the Word2Vec model.\n",
    "        \n",
    "        Parameters:\n",
    "        - vector_size (int): Dimensionality of word vectors.\n",
    "        - window_size (int): Context window size.\n",
    "        - learning_rate (float): Learning rate for gradient descent.\n",
    "        - epochs (int): Number of training iterations.\n",
    "        - negative_samples (int): Number of negative samples for each positive sample.\n",
    "        \"\"\"\n",
    "        self.vector_size = vector_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.negative_samples = negative_samples\n",
    "        self.vocab = {}\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.W = None  # Target word vectors\n",
    "        self.W_prime = None  # Context word vectors\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses the input text (tokenization, lowercasing).\n",
    "        \n",
    "        Parameters:\n",
    "        - text (str): The raw text string.\n",
    "        \n",
    "        Returns:\n",
    "        - tokens (list): A list of preprocessed tokens.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\W+', ' ', text)\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, corpus):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary from the corpus and initializes word vectors.\n",
    "        \n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents (each document is a string).\n",
    "        \"\"\"\n",
    "        tokens = list(chain.from_iterable([self.preprocess(doc) for doc in corpus]))\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for token in tokens:\n",
    "            self.word_counts[token] += 1\n",
    "        \n",
    "        # Build vocab with unique words and their indices\n",
    "        self.vocab = {word for word in self.word_counts}\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.index_to_word = {idx: word for word, idx in self.word_to_index.items()}\n",
    "        \n",
    "        # Initialize word vectors\n",
    "        vocab_size = len(self.vocab)\n",
    "        self.W = np.random.rand(vocab_size, self.vector_size)  # Target word vectors\n",
    "        self.W_prime = np.random.rand(vocab_size, self.vector_size)  # Context word vectors\n",
    "    \n",
    "    def generate_training_data(self, corpus):\n",
    "        \"\"\"\n",
    "        Generates training data (target-context pairs) for the Skip-gram model.\n",
    "        \n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents.\n",
    "        \n",
    "        Returns:\n",
    "        - training_data (list of tuples): A list of (target, context) word pairs.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for doc in corpus:\n",
    "            tokens = self.preprocess(doc)\n",
    "            for idx, word in enumerate(tokens):\n",
    "                target_idx = self.word_to_index[word]\n",
    "                start = max(0, idx - self.window_size)\n",
    "                end = min(len(tokens), idx + self.window_size + 1)\n",
    "                for context_word in tokens[start:idx] + tokens[idx+1:end]:\n",
    "                    pairs.append((target_idx, self.word_to_index[context_word]))\n",
    "        return pairs\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid of x.\n",
    "        \n",
    "        Parameters:\n",
    "        - x (float): The input value.\n",
    "        \n",
    "        Returns:\n",
    "        - sigmoid (float): Sigmoid output.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the Word2Vec model using the Skip-gram architecture and negative sampling.\n",
    "        \n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents.\n",
    "        \"\"\"\n",
    "        training_data = self.generate_training_data(corpus)\n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            loss = 0\n",
    "            for target_idx, context_idx in training_data:\n",
    "                # Positive sample\n",
    "                target_vector = self.W[target_idx]\n",
    "                context_vector = self.W_prime[context_idx]\n",
    "                \n",
    "                positive_score = self.sigmoid(np.dot(target_vector, context_vector))\n",
    "                loss += -np.log(positive_score)\n",
    "                \n",
    "                # Gradient update for positive sample\n",
    "                grad = self.learning_rate * (1 - positive_score)\n",
    "                self.W[target_idx] += grad * context_vector\n",
    "                self.W_prime[context_idx] += grad * target_vector\n",
    "                \n",
    "                # Negative sampling\n",
    "                for _ in range(self.negative_samples):\n",
    "                    negative_idx = random.randint(0, vocab_size - 1)\n",
    "                    if negative_idx == context_idx:\n",
    "                        continue\n",
    "                    negative_vector = self.W_prime[negative_idx]\n",
    "                    \n",
    "                    negative_score = self.sigmoid(-np.dot(target_vector, negative_vector))\n",
    "                    loss += -np.log(negative_score)\n",
    "                    \n",
    "                    # Gradient update for negative sample\n",
    "                    grad_neg = self.learning_rate * (1 - negative_score)\n",
    "                    self.W_prime[negative_idx] -= grad_neg * target_vector\n",
    "                    self.W[target_idx] -= grad_neg * negative_vector\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Retrieves the embedding for a given word.\n",
    "        \n",
    "        Parameters:\n",
    "        - word (str): The target word.\n",
    "        \n",
    "        Returns:\n",
    "        - embedding (numpy array): The word embedding vector.\n",
    "        \"\"\"\n",
    "        idx = self.word_to_index.get(word)\n",
    "        if idx is not None:\n",
    "            return self.W[idx]\n",
    "        else:\n",
    "            raise ValueError(f\"Word '{word}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 929.5796\n",
      "Epoch 2/10, Loss: 679.6919\n",
      "Epoch 3/10, Loss: 542.0443\n",
      "Epoch 4/10, Loss: 474.1075\n",
      "Epoch 5/10, Loss: 422.4208\n",
      "Epoch 6/10, Loss: 397.3668\n",
      "Epoch 7/10, Loss: 370.7108\n",
      "Epoch 8/10, Loss: 354.4496\n",
      "Epoch 9/10, Loss: 338.7151\n",
      "Epoch 10/10, Loss: 322.8259\n",
      "Embedding for 'quick':\n",
      "[ 0.35401816 -0.26008658  0.41378339 -0.18623707 -0.12941121 -0.25738425\n",
      "  0.1542014  -0.57640416 -0.52509664 -0.14805118]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train Word2Vec model\n",
    "w2v_model = Word2Vec(vector_size=10, window_size=2, epochs=10)\n",
    "w2v_model.build_vocab(corpus)\n",
    "w2v_model.train(corpus)\n",
    "\n",
    "# Retrieve embeddings\n",
    "embedding = w2v_model.get_embedding(\"quick\")\n",
    "print(f\"Embedding for 'quick':\\n{embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "What are the consequences of increasing the context window\n",
    "\n",
    "#### 1. Broader Contextual Information\n",
    "Effect: A larger window size means the model will consider more words around the target word as part of its context.\n",
    "\n",
    "Consequence:\n",
    "- Captures Broader Semantic Relationships:\n",
    "    - With a larger window, the model learns more about general topics or themes. For example, it can relate \"dog\" to \"park\" or \"walk\" even if they’re farther apart in a sentence.\n",
    "    - Useful for tasks that benefit from capturing document-level semantics, such as topic modeling or text classification.\n",
    "- Less Focus on Local Context:\n",
    "    - Smaller window sizes capture syntactic relationships (e.g., subject-verb-object), while larger windows focus more on semantic relationships (e.g., broader themes).\n",
    "\n",
    "#### 2. Increased Noise in Context\n",
    "Effect: With a larger window, more context words are included, but not all of them may be strongly related to the target word.\n",
    "\n",
    "Consequence:\n",
    "- Potential for Noisier Training Data:\n",
    "    - Words that are less relevant to the target (e.g., function words or distant topics) might dilute the training signal.\n",
    "    - For instance, in the sentence \"The quick brown fox jumps over the lazy dog,\" a window size of 5 might pair \"fox\" with \"dog,\" which is semantically weaker compared to \"jumps\" or \"brown.\"\n",
    "- Harder to Learn Precise Relationships:\n",
    "    - The model may struggle to distinguish between strongly and weakly related words if the window size includes too many irrelevant pairs.\n",
    "\n",
    "#### 3. Word Embedding Generalization\n",
    "Effect: Larger windows help capture high-level, general associations between words.\n",
    "\n",
    "Consequence:\n",
    "- Embeddings Capture Broader Themes:\n",
    "    - Word embeddings become more generalized, representing concepts that are coarser but applicable across broader contexts.\n",
    "    - Example: With a large window size, the embeddings for \"king\" and \"queen\" might show strong similarity not only due to gender-specific words in their immediate vicinity but also due to broader topics like \"monarchy\" or \"royalty.\"\n",
    "\n",
    "#### 4. Computational and Memory Costs\n",
    "Effect: More context words lead to more training pairs.\n",
    "\n",
    "Consequence:\n",
    "- Increased Computational Complexity:\n",
    "    - For each target word, a larger window size generates more (target, context) pairs, which means more updates during training.\n",
    "    - This increases the time and computational resources required for training.\n",
    "- Higher Memory Usage:\n",
    "    - Storing and processing a larger number of training pairs can consume more memory, especially in large corpora.\n",
    "\n",
    "#### 5. Impact on Skip-gram and CBOW\n",
    "Skip-gram:\n",
    "- Larger windows increase the number of context words it tries to predict for each target word.\n",
    "- Effect:\n",
    "    - Potentially more accurate embeddings for rare words due to increased training data.\n",
    "    - May result in noisier predictions for each context word if distant words are less related.\n",
    "CBOW:\n",
    "- Larger windows mean the model will aggregate more context words to predict the target.\n",
    "- Effect:\n",
    "    - May dilute the influence of strongly related context words.\n",
    "    - Requires efficient handling of larger input contexts during each training iteration.\n",
    "\n",
    "#### Task specific window size recommendations:\n",
    "\n",
    "| Task | Recomended size | Reason |\n",
    "| :---- | :-----: | :------ |\n",
    "|Syntactic analysis | Small (1-2) | Captures word order and grammatical roles |\n",
    "| Semantic similarity | Medium (3-5) | Balances local semantics with broader context |\n",
    "| Topic modelling | Large (5-10+) | Focuses on document-level themes |\n",
    "| Recommendation systems | Large (5-10+) | Captures weak signals and broad relationships |\n",
    "\n",
    "#### In practice\n",
    "Balancing window size in practice is domain specific, it is recommended to start small (2-3) and allow the model to focus on strong local relationships, then gradually increase the window size and evaluate how it impacts downstream tasks like classification, similarity, or clustering.\n",
    "\n",
    "With technical texts, smaller windows are prefered to capture precise relationships. Narrative texts may benifit from larger windows to capture themes and broader contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
