{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText, introduced by Facebook's AI Research (FAIR), extends Word2Vec by incorporating subword information into word embeddings. This makes FastText particularly useful for handling out-of-vocabulary (OOV) words and morphologically rich languages.\n",
    "\n",
    "#### Key Concepts of FastText\n",
    "##### Subword Representations:\n",
    "- Instead of treating each word as an atomic unit, FastText breaks words into n-grams (e.g., \"apple\" → <ap, app, ppl, ple, le> for n=3).\n",
    "- The word embedding is the sum of its n-gram embeddings.\n",
    "\n",
    "##### Training Objective:\n",
    "\n",
    "- Like Word2Vec, FastText supports Skip-gram and CBOW architectures.\n",
    "\n",
    "##### Advantages:\n",
    "- Handles OOV words by **summing the embeddings of their subwords**.\n",
    "- Improves performance on rare words by sharing information across similar subwords.\n",
    "\n",
    "\n",
    "##### Implementation Steps\n",
    "- Preprocess the text and build vocabulary.\n",
    "- Generate subword n-grams.\n",
    "- Train using the Skip-gram architecture.\n",
    "- Learn embeddings for words and subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "class FastText:\n",
    "    def __init__(self, vector_size=50, window_size=2, learning_rate=0.01, epochs=10, min_n=3, max_n=6, negative_samples=5):\n",
    "        \"\"\"\n",
    "        Initializes the FastText model.\n",
    "        \n",
    "        Parameters:\n",
    "        - vector_size (int): Dimensionality of word vectors.\n",
    "        - window_size (int): Context window size.\n",
    "        - learning_rate (float): Learning rate for gradient descent.\n",
    "        - epochs (int): Number of training iterations.\n",
    "        - min_n (int): Minimum length of subword n-grams.\n",
    "        - max_n (int): Maximum length of subword n-grams.\n",
    "        - negative_samples (int): Number of negative samples for each positive sample.\n",
    "        \"\"\"\n",
    "        self.vector_size = vector_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.min_n = min_n\n",
    "        self.max_n = max_n\n",
    "        self.negative_samples = negative_samples\n",
    "        self.vocab = {}\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.subword_to_index = {}\n",
    "        self.W = None  # Word vectors\n",
    "        self.W_sub = None  # Subword vectors\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses the input text (tokenization, lowercasing).\n",
    "        \n",
    "        Parameters:\n",
    "        - text (str): The raw text string.\n",
    "        \n",
    "        Returns:\n",
    "        - tokens (list): A list of preprocessed tokens.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\W+', ' ', text)\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, corpus):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary and subword n-grams.\n",
    "        \n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents (each document is a string).\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for doc in corpus:\n",
    "            tokens.extend(self.preprocess(doc))\n",
    "        \n",
    "        # Count word frequencies\n",
    "        word_counts = Counter(tokens)\n",
    "        \n",
    "        # Build vocab and initialize word-to-index mappings\n",
    "        self.word_to_index = {word: i for i, word in enumerate(word_counts)}\n",
    "        self.index_to_word = {i: word for word, i in self.word_to_index.items()}\n",
    "        self.vocab = set(self.word_to_index.keys())\n",
    "        \n",
    "        # Build subword n-grams\n",
    "        subwords = set()\n",
    "        for word in self.vocab:\n",
    "            subwords.update(self.get_subwords(word))\n",
    "        \n",
    "        self.subword_to_index = {sub: i for i, sub in enumerate(subwords)}\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        vocab_size = len(self.vocab)\n",
    "        subword_size = len(self.subword_to_index)\n",
    "        self.W = np.random.rand(vocab_size, self.vector_size)  # Word embeddings\n",
    "        self.W_sub = np.random.rand(subword_size, self.vector_size)  # Subword embeddings\n",
    "    \n",
    "    def get_subwords(self, word):\n",
    "        \"\"\"\n",
    "        Generates subword n-grams for a given word.\n",
    "        \n",
    "        Parameters:\n",
    "        - word (str): The input word.\n",
    "        \n",
    "        Returns:\n",
    "        - subwords (set): A set of subword n-grams.\n",
    "        \"\"\"\n",
    "        word = f\"<{word}>\"\n",
    "        subwords = set()\n",
    "        for n in range(self.min_n, self.max_n + 1):\n",
    "            for i in range(len(word) - n + 1):\n",
    "                subwords.add(word[i:i + n])\n",
    "        return subwords\n",
    "    \n",
    "    def generate_training_data(self, corpus):\n",
    "        \"\"\"\n",
    "        Generates training data (target-context pairs) for the Skip-gram model.\n",
    "        \n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents.\n",
    "        \n",
    "        Returns:\n",
    "        - training_data (list of tuples): A list of (target, context) word pairs.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for doc in corpus:\n",
    "            tokens = self.preprocess(doc)\n",
    "            for idx, word in enumerate(tokens):\n",
    "                start = max(0, idx - self.window_size)\n",
    "                end = min(len(tokens), idx + self.window_size + 1)\n",
    "                for context_word in tokens[start:idx] + tokens[idx+1:end]:\n",
    "                    pairs.append((word, context_word))\n",
    "        return pairs\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid of x.\n",
    "        \n",
    "        Parameters:\n",
    "        - x (float): The input value.\n",
    "        \n",
    "        Returns:\n",
    "        - sigmoid (float): Sigmoid output.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the FastText model using the Skip-gram architecture with subword embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents.\n",
    "        \"\"\"\n",
    "        training_data = self.generate_training_data(corpus)\n",
    "        subword_indices = {word: [self.subword_to_index[sub] for sub in self.get_subwords(word)] for word in self.vocab}\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            loss = 0\n",
    "            for target_word, context_word in training_data:\n",
    "                target_idx = self.word_to_index[target_word]\n",
    "                context_idx = self.word_to_index[context_word]\n",
    "                \n",
    "                # Get subword indices for the target word\n",
    "                target_subword_indices = subword_indices[target_word]\n",
    "                \n",
    "                # Compute target word embedding as the sum of its subword embeddings\n",
    "                target_embedding = np.sum(self.W_sub[target_subword_indices], axis=0)\n",
    "                \n",
    "                # Positive sample\n",
    "                context_embedding = self.W[context_idx]\n",
    "                positive_score = self.sigmoid(np.dot(target_embedding, context_embedding))\n",
    "                loss += -np.log(positive_score)\n",
    "                \n",
    "                # Gradient update for positive sample\n",
    "                grad = self.learning_rate * (1 - positive_score)\n",
    "                self.W[context_idx] += grad * target_embedding\n",
    "                for sub_idx in target_subword_indices:\n",
    "                    self.W_sub[sub_idx] += grad * context_embedding\n",
    "                \n",
    "                # Negative sampling\n",
    "                for _ in range(self.negative_samples):\n",
    "                    negative_idx = random.randint(0, len(self.vocab) - 1)\n",
    "                    negative_embedding = self.W[negative_idx]\n",
    "                    negative_score = self.sigmoid(-np.dot(target_embedding, negative_embedding))\n",
    "                    loss += -np.log(negative_score)\n",
    "                    \n",
    "                    # Gradient update for negative sample\n",
    "                    grad_neg = self.learning_rate * (1 - negative_score)\n",
    "                    for sub_idx in target_subword_indices:\n",
    "                        self.W_sub[sub_idx] -= grad_neg * negative_embedding\n",
    "                    self.W[negative_idx] -= grad_neg * target_embedding\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Retrieves the embedding for a given word, including OOV words.\n",
    "        \n",
    "        Parameters:\n",
    "        - word (str): The target word.\n",
    "        \n",
    "        Returns:\n",
    "        - embedding (numpy array): The word embedding vector.\n",
    "        \"\"\"\n",
    "        if word in self.word_to_index:\n",
    "            return self.W[self.word_to_index[word]]\n",
    "        else:\n",
    "            subword_indices = [self.subword_to_index[sub] for sub in self.get_subwords(word) if sub in self.subword_to_index]\n",
    "            if subword_indices:\n",
    "                return np.sum(self.W_sub[subword_indices], axis=0)\n",
    "            else:\n",
    "                raise ValueError(f\"Word '{word}' not in vocabulary or subword list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm trying to break down when and how subword embeddings are updated during FastText training in a step-by-step in an intuitive way:\n",
    "\n",
    "#### 1. Training in FastText is Word-Based\n",
    "\n",
    "FastText works on word-level training pairs, similar to Word2Vec. However, subword embeddings are used to calculate the word embeddings.\n",
    "\n",
    "For example, consider the sentence:\n",
    "\n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "\n",
    "With a context window of 2, one Skip-gram training pair could be:\n",
    "\n",
    "```\n",
    "(\"quick\",\"brown\")\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "Target word: quick\n",
    "Context word: brown\n",
    "\n",
    "#### 2. Word Embeddings Are Composed of Subword Embeddings\n",
    "\n",
    "**Subword N-grams**\n",
    "\n",
    "Let’s decompose each word into its subword n-grams:\n",
    "\n",
    "quick → `<qu`, `qui`, `uic`, `ick`, `k>`\n",
    "\n",
    "brown → `<br`, `bro`, `row`, `own`, `wn>`\n",
    "\n",
    "Each subword has its own **embedding vector**, initialized randomly.\n",
    "\n",
    "**Compute Word Embeddings**\n",
    "\n",
    "FastText computes the embedding of a word as the sum of its subword embeddings.\n",
    "\n",
    "Embedding of `quick`:\n",
    "```\n",
    "embedding(\"quick\")=embedding(\"<qu\")+embedding(\"qui\")+embedding(\"uic\")+embedding(\"ick\")+embedding(\"k>\")\n",
    "```\n",
    "\n",
    "Embedding of `brown`:\n",
    "\n",
    "```\n",
    "embedding(\"brown\")=embedding(\"<br\")+embedding(\"bro\")+embedding(\"row\")+embedding(\"own\")+embedding(\"wn>\")\n",
    "```\n",
    "\n",
    "#### 3. Skip-gram Training Objective\n",
    "\n",
    "The goal of Skip-gram is to maximize the probability of the context word (brown) given the target word (quick).\n",
    "\n",
    "Mathematically:\n",
    "```\n",
    "P(\"brown\"∣\"quick\")=σ(embedding(\"quick\")⋅embedding(\"brown\"))\n",
    "```\n",
    "\n",
    "Where, σ is the sigmoid function and ⋅ denotes the dot product.\n",
    "\n",
    "This tells the model how likely brown appears in the context of quick.\n",
    "\n",
    "#### 4. Loss and Gradient Calculation\n",
    "\n",
    "The model computes a loss based on how well it predicted brown given quick.\n",
    "\n",
    "- Step 1: Compute Prediction\n",
    "    - Compute the dot product of the target (quick) and context (brown) embeddings.\n",
    "    - Apply the sigmoid function to get the predicted probability.\n",
    "- Step 2: Compute Error\n",
    "    - Compare the predicted probability to the actual label (1 for correct pairs, 0 for negative samples).\n",
    "    - The difference (error) tells the model how much it needs to adjust the embeddings.\n",
    "- Step 3: Backpropagate the Error\n",
    "    - The error is backpropagated to update both:\n",
    "        - The subword embeddings of the target word (quick).\n",
    "        - The subword embeddings of the context word (brown).\n",
    "\n",
    "#### 5. Updating Subword Embeddings\n",
    "\n",
    "The update process involves gradient descent.\n",
    "\n",
    "Let’s focus on how the subwords of quick is updated:\n",
    "\n",
    "Target Word: `quick`\n",
    "\n",
    "The embedding of `quick` is composed of its subwords: `<qu`, `qui`, `uic`, `ick`, `k>`.\n",
    "\n",
    "Each subword contributes to the overall embedding of quick. During backpropagation:\n",
    "\n",
    "- The error calculated for quick is split among its subwords.\n",
    "- Each subword’s embedding is updated based on its contribution.\n",
    "\n",
    "For a subword s, its update rule looks like:\n",
    "\n",
    "```\n",
    "embedding(s)←embedding(s)+η⋅gradient\n",
    "```\n",
    "Where η is the learning rate. Gradient depends on the error and context embedding.\n",
    "\n",
    "Context Word: `brown`\n",
    "Similarly, the embedding of `brown` is composed of its subwords: `<br`, `bro`, `row`, `own`, `wn>`.\n",
    "\n",
    "Each subword in brown is updated based on the error signal from the target word (`quick`).\n",
    "\n",
    "#### 6. Intuition for Subword Updates\n",
    "\n",
    "- Subwords are shared across multiple words.\n",
    "    - For example, the subword `run` might appear in `running`, `runner`, and `runs`.\n",
    "    - Every time one of these words is involved in a training pair, the subword `run` is updated.\n",
    "    - Over time, frequent subwords (like `ing`) become well-optimized, while rare subwords improve slowly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
