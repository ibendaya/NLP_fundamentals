{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Implementation Steps\n",
    "1) Preprocess Text: Tokenize, lowercase, and optionally remove stopwords (similar to BoW).\n",
    "2) Calculate Term Frequency (TF): Frequency of a term in a document.\n",
    "3) Calculate Inverse Document Frequency (IDF): Measure of how unique a term is across the corpus.\n",
    "4) Combine TF and IDF: Multiply to get the TF-IDF score.\n",
    "5) Transform Corpus: Convert all documents into their TF-IDF vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "class TFIDF:\n",
    "    def __init__(self, stopwords=None):\n",
    "        \"\"\"\n",
    "        Initializes the TF-IDF model.\n",
    "\n",
    "        Parameters:\n",
    "        - stopwords (set): A set of words to ignore. Default is None.\n",
    "        \"\"\"\n",
    "        self.stopwords = stopwords if stopwords else set()\n",
    "        self.vocab = set()\n",
    "        self.doc_freq = defaultdict(int)  # Stores document frequency for each term\n",
    "        self.idf = {}  # Stores inverse document frequency for each term\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses the input text (tokenization, lowercasing, removing stopwords).\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The raw text string.\n",
    "\n",
    "        Returns:\n",
    "        - tokens (list): A list of preprocessed tokens.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\W+\", \" \", text)\n",
    "        tokens = text.split()\n",
    "        tokens = [word for word in tokens if word not in self.stopwords]\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab_and_idf(self, corpus):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary and computes the IDF values from a corpus.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents (each document is a string).\n",
    "        \"\"\"\n",
    "        num_docs = len(corpus)\n",
    "        for document in corpus:\n",
    "            tokens = set(\n",
    "                self.preprocess(document)\n",
    "            )  # Use set to count unique tokens in a document\n",
    "            for token in tokens:\n",
    "                self.vocab.add(token)\n",
    "                self.doc_freq[token] += 1\n",
    "\n",
    "        # Compute IDF for each term\n",
    "        self.idf = {\n",
    "            term: math.log((1 + num_docs) / (1 + self.doc_freq[term])) + 1\n",
    "            for term in self.vocab\n",
    "        }\n",
    "\n",
    "    def compute_tf(self, document):\n",
    "        \"\"\"\n",
    "        Computes the term frequency (TF) for a single document.\n",
    "\n",
    "        Parameters:\n",
    "        - document (str): A single document (string).\n",
    "\n",
    "        Returns:\n",
    "        - tf (dict): A dictionary of term frequencies.\n",
    "        \"\"\"\n",
    "        tokens = self.preprocess(document)\n",
    "        token_counts = Counter(tokens)\n",
    "        total_tokens = len(tokens)\n",
    "        tf = {term: count / total_tokens for term, count in token_counts.items()}\n",
    "        return tf\n",
    "\n",
    "    def vectorize(self, document):\n",
    "        \"\"\"\n",
    "        Converts a document into its TF-IDF vector representation.\n",
    "\n",
    "        Parameters:\n",
    "        - document (str): A single document (string).\n",
    "\n",
    "        Returns:\n",
    "        - vector (dict): A dictionary representing the TF-IDF vector for the\n",
    "        document.\n",
    "        \"\"\"\n",
    "        tf = self.compute_tf(document)\n",
    "        tfidf_vector = {\n",
    "            term: tf.get(term, 0) * self.idf.get(term, 0) for term in self.vocab\n",
    "        }\n",
    "        return tfidf_vector\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        \"\"\"\n",
    "        Transforms a corpus into TF-IDF vectors.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus (list of str): A list of documents.\n",
    "\n",
    "        Returns:\n",
    "        - vectors (list of dict): A list of TF-IDF vectors for each document.\n",
    "        \"\"\"\n",
    "        return [self.vectorize(doc) for doc in corpus]\n",
    "\n",
    "    def vector_to_dense(self, vector):\n",
    "        \"\"\"\n",
    "        Converts a sparse TF-IDF vector to a dense list of scores.\n",
    "\n",
    "        Parameters:\n",
    "        - vector (dict): A sparse dictionary representing the TF-IDF vector.\n",
    "\n",
    "        Returns:\n",
    "        - dense_vector (list): A dense list of TF-IDF scores.\n",
    "        \"\"\"\n",
    "        return [vector[term] for term in sorted(self.vocab)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 TF-IDF Vector: [0.2821911967599909, 0.21461367874196347, 0.0, 0.2821911967599909, 0.0, 0.0, 0.0, 0.2821911967599909, 0.21461367874196347, 0.0, 0.0, 0.0, 0.21461367874196347, 0.0, 0.0]\n",
      "Document 2 TF-IDF Vector: [0.0, 0.2575364144903562, 0.0, 0.0, 0.0, 0.0, 0.3386294361119891, 0.0, 0.2575364144903562, 0.0, 0.3386294361119891, 0.0, 0.0, 0.3386294361119891, 0.0]\n",
      "Document 3 TF-IDF Vector: [0.0, 0.0, 0.24187816865142076, 0.0, 0.24187816865142076, 0.24187816865142076, 0.0, 0.0, 0.0, 0.24187816865142076, 0.0, 0.24187816865142076, 0.18395458177882582, 0.0, 0.24187816865142076]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Never jump over the lazy dog quickly\",\n",
    "    \"A quick movement of the enemy will jeopardize six gunboats\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF model with stopwords\n",
    "stopwords = {\"the\", \"over\", \"a\", \"will\"}\n",
    "tfidf_model = TFIDF(stopwords=stopwords)\n",
    "\n",
    "# Build vocabulary and compute IDF values\n",
    "tfidf_model.build_vocab_and_idf(corpus)\n",
    "\n",
    "# Transform the corpus into TF-IDF vectors\n",
    "vectors = tfidf_model.transform(corpus)\n",
    "\n",
    "# Display dense TF-IDF vectors\n",
    "for i, vector in enumerate(vectors):\n",
    "    dense_vector = tfidf_model.vector_to_dense(vector)\n",
    "    print(f\"Document {i+1} TF-IDF Vector: {dense_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How TF-IDF Improves on Bag of Words (BoW)\n",
    "Both Bag of Words (BoW) and TF-IDF are methods to represent text data numerically. However, TF-IDF addresses several limitations of the basic BoW approach. Here's how TF-IDF improves upon BoW:\n",
    "\n",
    "#### 1. Weighting Term Importance\n",
    "BoW:\n",
    "- Counts the frequency of each word in a document.\n",
    "- Treats all words equally, regardless of their importance or relevance.\n",
    "\n",
    "Example: Common words like \"the\", \"is\", and \"and\" are treated with the same importance as more meaningful words like \"data\", \"model\", or \"accuracy\".\n",
    "\n",
    "TF-IDF:\n",
    "- Assigns weights to words based on their importance.\n",
    "  - TF (Term Frequency): Measures how often a word appears in a document.\n",
    "  - IDF (Inverse Document Frequency): Reduces the weight of common words that appear in many documents (e.g., \"the\", \"is\") and increases the weight of rarer, more informative words.\n",
    "\n",
    "Benefit:\n",
    "- TF-IDF emphasizes important terms that are specific to a document, reducing the influence of frequently occurring but less informative terms.\n",
    "\n",
    "#### 2. Reducing the Impact of Common Words\n",
    "BoW:\n",
    "- Highly frequent words (stopwords) dominate the representation.\n",
    "- In many documents, words like \"the\" or \"and\" have the highest counts, even though they provide little information about the document's content.\n",
    "\n",
    "TF-IDF:\n",
    "- By calculating IDF, common words that appear in most documents receive lower weights.\n",
    "- Rare or unique terms get higher weights, improving the modelâ€™s ability to distinguish between documents based on specific content.\n",
    "\n",
    "Example:\n",
    "- In a BoW vector, \"the\" might have a high frequency in most documents.\n",
    "- In TF-IDF, the IDF component reduces the weight of \"the\", making unique words like \"machine\" or \"learning\" more influential.\n",
    "\n",
    "#### 3. Handling Document Length Bias\n",
    "BoW:\n",
    "- Longer documents tend to have higher word frequencies, resulting in larger BoW vectors. This can skew the similarity between documents, favoring longer ones.\n",
    "\n",
    "TF-IDF:\n",
    "- Normalizes the term frequency by the document length.\n",
    "  - TF scales each term frequency relative to the total number of terms in the document.\n",
    "  - This makes the representation independent of document length, allowing fairer comparisons between short and long documents.\n",
    "\n",
    "#### 4. Improving Document Similarity Calculations\n",
    "BoW:\n",
    "- Cosine Similarity or other distance metrics on BoW vectors may be dominated by common terms.\n",
    "- Two documents about unrelated topics may appear similar if they both contain common words like \"the\" or \"is\".\n",
    "\n",
    "TF-IDF:\n",
    "- Reduces the impact of common terms in similarity calculations.\n",
    "- Documents are compared based on the weight of informative, discriminative words, leading to more meaningful similarity scores.\n",
    "\n",
    "#### 5. Applications in Search and Ranking\n",
    "BoW:\n",
    "- Treats all terms equally, so search systems might return irrelevant results if query terms are common.\n",
    "\n",
    "TF-IDF:\n",
    "- Improves search engines by ranking documents higher if they contain query terms with high TF-IDF scores.\n",
    "\n",
    "Example: A search for \"machine learning\" will prioritize documents where these terms are significant, not just frequent.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
